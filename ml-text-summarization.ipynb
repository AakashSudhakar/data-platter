{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Text Summarization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 0: Imports and Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# NLTK modules\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Tensorflow modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Miscellaneous utilities\n",
    "import numpy as np\n",
    "import pandas as pd; pd.set_option(\"display.max_colwidth\", 200)\n",
    "import re\n",
    "import os\n",
    "import bs4 as bs\n",
    "from urllib import urlopen\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Silence miscellaneous warnings\n",
    "import warnings; warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_modules import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we initialize our data processing engine for miscellaneous text data online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/aakashsudhakar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aakashsudhakar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading essential data from NLTK\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Overview of Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Major Types of Text Summarization:\n",
    "    - Extraction-based summarization\n",
    "    - Abstraction-based summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to Perform Text Summarization:\n",
    "    1. Convert the paragraph into sentences.\n",
    "    2. Perform text processing.\n",
    "    3. Perform tokenization.\n",
    "    4. Evaluated the weighted occurrence frequency of the words. \n",
    "    5. Substitute words with their weighted frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "![](https://paper-attachments.dropbox.com/s_5DD7360138DEDEB8828AD11E4B5921DC0A55833560A1BC79C451FADB6E7D209D_1554467410003_image.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Breakdown of Code Constructs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = \"https://en.wikipedia.org/wiki/20th_century\"\n",
    "\n",
    "data_read = urlopen(PATH_DATA).read()\n",
    "data_parsed = bs.BeautifulSoup(data_read, \"html.parser\")\n",
    "\n",
    "data_paragraphs = data_parsed.find_all(\"p\")\n",
    "\n",
    "data_content = str()\n",
    "for paragraph in paragraphs:\n",
    "    data_content += paragraph.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_frequency_table(text):\n",
    "    \"\"\" Function to create frequency histogram of word occurrences across input text. \"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    raw_words_from_data = word_tokenize(text)\n",
    "    stem = PorterStemmer()\n",
    "    # Create frequency table via dictionary operations\n",
    "    frequency_table = dict()\n",
    "    for word in raw_words_from_data:\n",
    "        word_root = stem.stem(word)\n",
    "        if word_root in stop_words:\n",
    "            continue\n",
    "        if word_root in frequency_table:\n",
    "            frequency_table[word_root] += 1\n",
    "        else:\n",
    "            frequency_table[word_root] = 1\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Tokenize the article into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(data_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Find the weighted frequencies of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_scores(sentences, frequency_table, num_chars=7):\n",
    "    \"\"\" Function to create weighted frequency scores from parsed sentences using frequency table. \"\"\"\n",
    "    sentence_weight = dict()\n",
    "    for sentence in sentences:\n",
    "        sentence_wordcount_without_stop_words = 0\n",
    "        sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "        for word_weight in frequency_table:\n",
    "            if word_weight in sentence.lower():\n",
    "                sentence_wordcount_without_stop_words += 1\n",
    "                if sentence[:num_chars] in sentence_weight:\n",
    "                    sentence_weight[sentence[:num_chars]] += frequency_table[word_weight]\n",
    "                else:\n",
    "                    sentence_weight[sentence[:num_chars]] = frequency_table[word_weight]\n",
    "        sentence_weight[sentence[:num_chars]] /= sentence_wordcount_without_stop_words\n",
    "    return sentence_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Calculate the threshold of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_threshold(sentence_weight):\n",
    "    \"\"\" Function to get the average weighted score of a sentence. \"\"\"\n",
    "    sum_values = 0\n",
    "    for element in sentence_weight:\n",
    "        sum_values += sentence_weight[element]\n",
    "    return (sum_values / len(sentence_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Obtain the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_summary(sentences, sentence_weight, threshold, num_chars=7):\n",
    "    \"\"\" Function to create summary statement of article using weighted sentence data and relative threshold. \"\"\"\n",
    "    sentence_counter, article_summary = 0, str()\n",
    "    for sentence in sentences:\n",
    "        if sentence[:num_chars] in sentence_weight and sentence_weight[sentence[:num_chars]] >= (threshold):\n",
    "            article_summary += \" {}\".format(sentence)\n",
    "            sentence_counter += 1\n",
    "    return article_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Analyzing our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap this all up into a nice outer function and run our summarization analysis on our sample Wikipedia and check our results!\n",
    "\n",
    "Since this is extraction-based, it won't be nearly as nicely grammatical and well-structured as an abstraction-based (deep learning and advanced modeling) approach, but it should be sufficient to give us an adequate summary of the article's topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text_summary(text):\n",
    "    frequency_table = create_frequency_table(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_scores = calculate_sentence_scores(sentences, frequency_table)\n",
    "    threshold = calculate_average_threshold(sentence_scores)\n",
    "    text_summary = get_text_summary(sentences, sentence_scores, 1.5 * threshold)\n",
    "    return text_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Terms like ideology, world war, genocide, and nuclear war entered common usage. Humans explored space for the first time, taking their first footsteps on the Moon. However, these same wars resulted in the destruction of the imperial system. The victorious Bolsheviks then established the Soviet Union, the world's first communist state. At the beginning of the period, the British Empire was the world's most powerful nation,[12] having acted as the world's policeman for the past century. In total, World War II left some 60 million people dead. With the Axis defeated and Britain and France rebuilding, the United States and the Soviet Union were left standing as the world's only superpowers. At the beginning of the century, strong discrimination based on race and sex was significant in general society. During the century, the social taboo of sexism fell. Communications and information technology, transportation technology, and medical advances had radically altered daily lives. With the end of colonialism and the Cold War, nearly a billion people in Africa were left in new nation states after centuries of foreign domination. Since the US was in a dominant position, a major part of the process was Americanization. Terrorism, dictatorship, and the spread of nuclear weapons were pressing global issues. Millions were infected with HIV, the virus which causes AIDS. This includes deaths caused by wars, genocide, politicide and mass murders. Prior to the 20th century, music was generally only experienced in live performances. Later in the 20th century, the development of computers led to the establishment of a theory of computation.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_text_summary(data_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Constructing a Higher-Order Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Summarization_Engine(object):\n",
    "    \"\"\" Class instance for producing extraction-based summaries from input corpus data. \"\"\"\n",
    "    def __init__(self, query=None):\n",
    "        self.num_chars = 7\n",
    "        if query is None:\n",
    "            self.path = \"https://en.wikipedia.org/wiki/Randomness\"\n",
    "        else:\n",
    "            self.path = \"https://en.wikipedia.org/wiki/{}\".format(query)\n",
    "        self.dataset = self._process_data()\n",
    "            \n",
    "    def _process_data(self):\n",
    "        \"\"\" Instance method to load, clean, and parse linguistic data from raw text corpus. \"\"\"\n",
    "        data_read = urlopen(self.path).read()\n",
    "        data_parsed = bs.BeautifulSoup(data_read, \"html.parser\")\n",
    "        data_paragraphs = data_parsed.find_all(\"p\")\n",
    "        data_content = str()\n",
    "        for paragraph in data_paragraphs:\n",
    "            data_content += paragraph.text\n",
    "        return data_content\n",
    "            \n",
    "    def _create_frequency_table(self):\n",
    "        \"\"\" Instance method to create frequency histogram of word occurrences across input text. \"\"\"\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        data_words, stem = word_tokenize(self.dataset), PorterStemmer()\n",
    "        stem, frequency_table = PorterStemmer(), dict()\n",
    "        for word in data_words:\n",
    "            word_root = stem.stem(word)\n",
    "            if word_root in stop_words:\n",
    "                continue\n",
    "            if word_root in frequency_table:\n",
    "                frequency_table[word_root] += 1\n",
    "            else:\n",
    "                frequency_table[word_root] = 1\n",
    "        return frequency_table\n",
    "    \n",
    "    def _calculate_sentence_weights(self, frequency_table, sentences):\n",
    "        \"\"\" Instance method to create weighted frequency scores from parsed sentences using frequency table. \"\"\"\n",
    "        sentence_weights = dict()\n",
    "        for sentence in sentences:\n",
    "            sentence_wordcount_without_stop_words = 0\n",
    "            sentence_wordcount = (len(word_tokenize(sentence)))\n",
    "            for word_weight in frequency_table:\n",
    "                if word_weight in sentence.lower():\n",
    "                    sentence_wordcount_without_stop_words += 1\n",
    "                    if sentence[:self.num_chars] in sentence_weights:\n",
    "                        sentence_weights[sentence[:self.num_chars]] += frequency_table[word_weight]\n",
    "                    else:\n",
    "                        sentence_weights[sentence[:self.num_chars]] = frequency_table[word_weight]\n",
    "            sentence_weights[sentence[:self.num_chars]] /= sentence_wordcount_without_stop_words\n",
    "        return sentence_weights\n",
    "    \n",
    "    def _calculate_average_threshold(self, sentence_weights):\n",
    "        \"\"\" Instance method to get the average weight across all sentences. \"\"\"\n",
    "        return sum(sentence_weights.values()) / len(sentence_weights)\n",
    "    \n",
    "    def _get_text_summary(self, sentences, sentence_weights, relative_threshold):\n",
    "        \"\"\" Instance method to create summary statement of corpus using weighted sentence data and relative threshold. \"\"\"\n",
    "        sentence_counter, text_summary = 0, str()\n",
    "        for sentence in sentences:\n",
    "            if sentence[:self.num_chars] in sentence_weights and sentence_weights[sentence[:self.num_chars]] >= (relative_threshold):\n",
    "                text_summary += \" {}\\n\".format(sentence.encode(\"utf-8\"))\n",
    "                sentence_counter += 1\n",
    "        return text_summary\n",
    "    \n",
    "    def run_text_summarization(self):\n",
    "        \"\"\" Instance method to perform end-to-end text summarization analysis on parsed dataset. \"\"\"\n",
    "        frequency_table, sentences = self._create_frequency_table(), sent_tokenize(self.dataset)\n",
    "        sentence_weights = self._calculate_sentence_weights(frequency_table, sentences)\n",
    "        threshold = self._calculate_average_threshold(sentence_weights)\n",
    "        return self._get_text_summary(sentences, sentence_weights, 1.5 * threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create terse formatting script for basic search queries in Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_search_query(query):\n",
    "    \"\"\" Global function that formats and restructures basic search query from user to Wikipedia search. \"\"\"\n",
    "    return \" \".join(word.capitalize() for word in query.split()).replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Greek philosophers discussed randomness at length, but only in non-quantitative forms.\n",
      " In the 1888 edition of his book The Logic of Chance, John Venn wrote a chapter on The conception of randomness that included his view of the randomness of the digits of pi, by using them to construct a random walk in two dimensions.\n",
      " In the mid- to late-20th century, ideas of algorithmic information theory introduced new dimensions to the field via the concept of algorithmic randomness.\n",
      " In the first six billion decimal places of pi, each of the digits from 0 through 9 shows up about six hundred million times.\n",
      " [17]In statistics, randomness is commonly used to create simple random samples.\n",
      " Noise consists of numerous transient disturbances, with a statistically randomized time distribution.\n",
      " If the universe is regarded to have a purpose, then randomness can be seen as impossible.\n",
      " In fact, randomness has been used for games of chance throughout history, and to select out individuals for an unwanted task in a fair way (see drawing straws).\n",
      " There are many practical measures of randomness for a binary sequence.\n",
      " In fact, there is no finite number of trials that can guarantee a success.\n",
      " If the die is fair, then previous rolls can give no indication of future events.\n",
      " In the beginning of a scenario, one might calculate the probability of a certain event.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Input user-defined search query in Wikipedia here. \n",
    "query = \"software engineering\"\n",
    "\n",
    "# Search query is refined in global formatting function.\n",
    "query = format_search_query(query)\n",
    "\n",
    "# Instantiate text summarization processor with user-defined search query\n",
    "proc = Text_Summarization_Engine()\n",
    "\n",
    "# Produce summary of relevant Wikipedia article\n",
    "print(proc.run_text_summarization())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: Introducing Abstractive Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractive summarization, unlike its extractive cousin, utilize **generative** training models to approximate and create new sentences from scratch rather than recombining old tokens from prior sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/05/abstractive1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With text summarization, our input and output are the same: sequences of words. We can utilize `Seq2Seq` model architectures within the realm of deep learning to attack this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/05/final.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Seq2Seq` models utilize *Encoder-Decoder* architectures to resolve the issue of encoding information and decoding results being of differing lengths. \n",
    "\n",
    "These architectures often use RNNs (Recurrent Neural Networks) and/or LSTMs (Long Short Term Memories) due to their proclivity for understanding long-term dependencies across sequential data. We'll be working with LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/06/first.jpg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder-decoder has two primary phases: **training** and **interference**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an encoding LSTM, a word is inputted at each timestep into the pipeline and interpreted at every timestep across its parent sequence to better understand the context of the word. This way, the entire input sequence is interpreted both in short-term and longetive context. \n",
    "\n",
    "The final state of the encoder receives the hidden state (<i>h</i>) and cell state (<i>c</i>) weights that instantiate the decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/05/61.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decoding LSTM, the target sequence is fed piecewise through the network, which attempts to predict the same sequence offset by a single timestep. In this way, the decoder will be predictive of each subsequent word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/05/71.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interference Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interference phase architecture is largely similar to our training phase architecture, with the critical difference that input sequences are now **independent** of a target sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/05/82.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interference process can be outlined as a multi-step mechanism as follows:\n",
    "    1. Encode the input sequence and instantiate the decoder with the final hidden and cell states of the encoder.\n",
    "    2. Pass the *[START]* token as the first input of the decoder.\n",
    "    3. Run the decoder for one timestep. (Output will be the probability for next word; word with highest probability is selected.)\n",
    "    4. Pass sampled word as new decoder input with updated hidden and cell states from previous timestep.\n",
    "    5. Repeat steps 3-4 until *[STOP]* token or maximum target sequence length is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder-decoder has one critical limitation: *it is unable to perform effectively with longer and longer sequences*.\n",
    "\n",
    "This is where we make use of the **attention mechanism**. \n",
    "\n",
    "The attention mechanism modifies relative importance given to each word of an input sequence based on how relevant it is towards predicting the target sequence. In other words, it works so that the algorithm as a whole only has to look at a small set of words across a sequence rather than each word to predict an effective response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism has two primary classes: **global attention** and **local attention**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hidden states are considered for deriving the next context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only some hidden states are considered for deriving the next context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working primarily with a *global attention mechanism*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 6: Diving into Abstractive Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH, NROWS = \"./datasets/amazon-fine-food-reviews/Reviews.csv\", 1e5\n",
    "\n",
    "dataset = pd.read_csv(FILEPATH, nrows=NROWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we'll immediately get rid of those pesky `NaN` values and other erroneous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop_duplicates(subset=[\"Text\"], inplace=True)\n",
    "dataset.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do some data preprocessing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \n",
    "                       \"aren't\": \"are not\",\n",
    "                       \"can't\": \"cannot\", \n",
    "                       \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\",\n",
    "                       \"didn't\": \"did not\", \n",
    "                       \"doesn't\": \"does not\", \n",
    "                       \"don't\": \"do not\", \n",
    "                       \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\",\n",
    "                       \"he'd\": \"he would\",\n",
    "                       \"he'll\": \"he will\", \n",
    "                       \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \n",
    "                       \"how'd'y\": \"how do you\", \n",
    "                       \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",\n",
    "                       \"I'd\": \"I would\", \n",
    "                       \"I'd've\": \"I would have\", \n",
    "                       \"I'll\": \"I will\", \n",
    "                       \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\", \n",
    "                       \"I've\": \"I have\", \n",
    "                       \"i'd\": \"i would\",\n",
    "                       \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \n",
    "                       \"i'll've\": \"i will have\",\n",
    "                       \"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \n",
    "                       \"it'd\": \"it would\",\n",
    "                       \"it'd've\": \"it would have\", \n",
    "                       \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \n",
    "                       \"let's\": \"let us\", \n",
    "                       \"ma'am\": \"madam\",\n",
    "                       \"mayn't\": \"may not\", \n",
    "                       \"might've\": \"might have\",\n",
    "                       \"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \n",
    "                       \"must've\": \"must have\",\n",
    "                       \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \n",
    "                       \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\",\n",
    "                       \"oughtn't\": \"ought not\", \n",
    "                       \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \n",
    "                       \"sha'n't\": \"shall not\", \n",
    "                       \"shan't've\": \"shall not have\",\n",
    "                       \"she'd\": \"she would\", \n",
    "                       \"she'd've\": \"she would have\", \n",
    "                       \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \n",
    "                       \"she's\": \"she is\",\n",
    "                       \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \n",
    "                       \"shouldn't've\": \"should not have\", \n",
    "                       \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\",\n",
    "                       \"this's\": \"this is\",\n",
    "                       \"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \n",
    "                       \"that's\": \"that is\", \n",
    "                       \"there'd\": \"there would\",\n",
    "                       \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \n",
    "                       \"they'd've\": \"they would have\",\n",
    "                       \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \n",
    "                       \"they're\": \"they are\", \n",
    "                       \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\",\n",
    "                       \"wasn't\": \"was not\", \n",
    "                       \"we'd\": \"we would\", \n",
    "                       \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \n",
    "                       \"we'll've\": \"we will have\", \n",
    "                       \"we're\": \"we are\",\n",
    "                       \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\", \n",
    "                       \"what'll\": \"what will\", \n",
    "                       \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",\n",
    "                       \"what's\": \"what is\", \n",
    "                       \"what've\": \"what have\", \n",
    "                       \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \n",
    "                       \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\",\n",
    "                       \"where've\": \"where have\", \n",
    "                       \"who'll\": \"who will\", \n",
    "                       \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\",\n",
    "                       \"why's\": \"why is\", \n",
    "                       \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \n",
    "                       \"won't\": \"will not\", \n",
    "                       \"won't've\": \"will not have\",\n",
    "                       \"would've\": \"would have\", \n",
    "                       \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\",\n",
    "                       \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\n",
    "                       \"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \n",
    "                       \"you'll\": \"you will\", \n",
    "                       \"you'll've\": \"you will have\",\n",
    "                       \"you're\": \"you are\", \n",
    "                       \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to perform the following preprocessing tasks:\n",
    "    - Convert everything to lowercase.\n",
    "    - Remove HTML tags.\n",
    "    - Contraction mapping.\n",
    "    - Remove ('s). \n",
    "    - Remove any text inside parentheses.\n",
    "    - Eliminate punctuation and special characters.\n",
    "    - Remove stop words.\n",
    "    - Remove short words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = bs.BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    text = re.sub('\"','', text)\n",
    "    text = \" \".join([contraction_mapping[element] if element in contraction_mapping else element for element in text.split(\" \")])    \n",
    "    text = re.sub(r\"'s\\b\",\"\",text)\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    tokens = [word for word in text.split() if not word in stop_words]\n",
    "    long_words = list()\n",
    "    \n",
    "    for token in tokens:\n",
    "        if len(token) >= 3:\n",
    "            long_words.append(token)\n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n",
    "text_cleaned = list()\n",
    "for item in dataset[\"Text\"]:\n",
    "    text_cleaned.append(clean_text(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            Good Quality Dog Food\n",
       "1                                Not as Advertised\n",
       "2                            \"Delight\" says it all\n",
       "3                                   Cough Medicine\n",
       "4                                      Great taffy\n",
       "5                                       Nice Taffy\n",
       "6    Great!  Just as good as the expensive brands!\n",
       "7                           Wonderful, tasty taffy\n",
       "8                                       Yay Barley\n",
       "9                                 Healthy Dog Food\n",
       "Name: Summary, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"Summary\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
